var documenterSearchIndex = {"docs":
[{"location":"internal/#Internal-API-(Unexported)","page":"Internal API (Unexported)","title":"Internal API (Unexported)","text":"","category":"section"},{"location":"internal/","page":"Internal API (Unexported)","title":"Internal API (Unexported)","text":"The following symbols are internal helpers or implementation details. They are documented here for contributors and advanced users. These APIs may change without notice.","category":"page"},{"location":"internal/#PyBayesOpt._toscale!","page":"Internal API (Unexported)","title":"PyBayesOpt._toscale!","text":"_toscale!(points::Matrix, state::BoTorchQBatchState)\n_toscale!(point::Vector, state::BoTorchQBatchState)\n\nScale point(s) from [0,1]^d to problem bounds.\n\n\n\n\n\n","category":"function"},{"location":"internal/#PyBayesOpt._to01!","page":"Internal API (Unexported)","title":"PyBayesOpt._to01!","text":"_to01!(points::Matrix, state::BoTorchQBatchState)\n_to01!(point::Vector, state::BoTorchQBatchState)\n\nScale point(s) from problem bounds to  [0,1]^d.\n\n\n\n\n\n","category":"function"},{"location":"internal/#PyBayesOpt.dict2vec","page":"Internal API (Unexported)","title":"PyBayesOpt.dict2vec","text":"dict2vec(d)\n\nConvert a dictionary with string keys \"x001\", \"x002\", ... to a vector.\n\n\n\n\n\n","category":"function"},{"location":"internal/#PyBayesOpt.vec2pairs","page":"Internal API (Unexported)","title":"PyBayesOpt.vec2pairs","text":"vec2pairs(v)\n\nConvert a vector to pairs suitable for the BayesianOptimization constructor.\n\n\n\n\n\n","category":"function"},{"location":"internal/#PyBayesOpt.bounds2pairs","page":"Internal API (Unexported)","title":"PyBayesOpt.bounds2pairs","text":"bounds2pairs(bounds::Matrix)\n\nConvert a bounds matrix to pairs suitable for the BayesianOptimization constructor.\n\n\n\n\n\n","category":"function"},{"location":"internal/#PyBayesOpt.x00i","page":"Internal API (Unexported)","title":"PyBayesOpt.x00i","text":"x00i(i)\n\nGenerate variable name in format \"x001\", \"x002\", etc. for variable index i.\n\n\n\n\n\n","category":"function"},{"location":"internal/#PyBayesOpt.get","page":"Internal API (Unexported)","title":"PyBayesOpt.get","text":"get(d::Dict, i)\n\nGet the i-th variable from dictionary d using the naming convention \"x001\", \"x002\", etc.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.jl-Documentation","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"Julia wrapper for Bayesian optimization tools available in Python.","category":"page"},{"location":"#Overview","page":"PyBayesOpt.jl Documentation","title":"Overview","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"PyBayesOpt.jl provides Julia interfaces to popular Python Bayesian optimization libraries:","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"BayesianOptimization - A Python library for Bayesian optimization\nBoTorch - A Bayesian optimization library built on PyTorch for q-batch optimization","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"The package implements the Optim.jl interface for seamless integration with Julia's optimization ecosystem.","category":"page"},{"location":"#Quick-Start","page":"PyBayesOpt.jl Documentation","title":"Quick Start","text":"","category":"section"},{"location":"#Basic-BoTorch-Q-Batch-Optimization","page":"PyBayesOpt.jl Documentation","title":"Basic BoTorch Q-Batch Optimization","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"using PyBayesOpt\nusing Optim\n\n# Define a test function to minimize\nobjective(x) = (x[1] - 2)^2 + (x[2] - 1)^2\n\n# Set up optimization parameters\nparams = BoTorchQBatch(\n    bounds = [-5.0 5.0; -5.0 5.0],  # [min max] for each dimension  \n    nbatch = 4,     # batch size\n    ninit = 10,     # initialization iterations\n    nopt = 20       # optimization iterations\n)\n\n# Run optimization using Optim.jl interface\nresult = optimize(objective, params)\n\n# Access results\nprintln(\"Best point: \", result.minimizer)\nprintln(\"Best value: \", result.minimum)\nprintln(\"Function evaluations: \", result.f_calls)","category":"page"},{"location":"#BayesianOptimization-Wrapper","page":"PyBayesOpt.jl Documentation","title":"BayesianOptimization Wrapper","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"# Using the BayesianOptimization wrapper\nparams = BayesianOptimization(\n    bounds = [-5.0 5.0; -5.0 5.0],\n    ninit = 10,\n    nopt = 50\n)\n\nresult = optimize(objective, params)","category":"page"},{"location":"#Interactive-Optimization-Loop","page":"PyBayesOpt.jl Documentation","title":"Interactive Optimization Loop","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"For more control over the optimization process:","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"# Interactive optimization with BoTorch\nstate = BoTorchQBatchState(params=BoTorchQBatch(bounds=[-5.0 5.0; -5.0 5.0]))\n\nwhile !finished(state)\n    # Get next batch of points to evaluate\n    pts = ask!(state)\n    \n    # Evaluate function (can be parallelized)\n    values = [objective(pts[:, i]) for i in 1:size(pts, 2)]\n    \n    # Provide results back to optimizer\n    tell!(state, pts, values)\n    \n    # Check current best\n    if state.optimization_complete\n        best_pt, best_val = bestpoint(state)\n        println(\"Current best: $best_pt -> $best_val\")\n    end\nend","category":"page"},{"location":"#Benchmark-Functions","page":"PyBayesOpt.jl Documentation","title":"Benchmark Functions","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"The package includes several standard optimization benchmarks:","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"# Use benchmark functions\nbranin = BraninFunction()\nresult = optimize(branin, BoTorchQBatch(bounds=branin.bounds))\n\nackley = AckleyFunction(dim=3)  # 3D Ackley function\nresult = optimize(ackley, BoTorchQBatch(bounds=ackley.bounds))\n\nrosenbrock = RosenbrockFunction(dim=2)\nresult = optimize(rosenbrock, BoTorchQBatch(bounds=rosenbrock.bounds))","category":"page"},{"location":"#Using-Optim.jl-Interface","page":"PyBayesOpt.jl Documentation","title":"Using Optim.jl Interface","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"PyBayesOpt implements the Optim.jl optimize function interface, making it a drop-in replacement for other Optim.jl optimizers:","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"using Optim\nusing PyBayesOpt\n\n# Your objective function\nfunction objective(x)\n    return sum((x .- [2.0, 3.0]).^2)  # minimize at [2.0, 3.0]\nend\n\n# Create method instance\nmethod = BoTorchQBatch(\n    bounds = [0.0 5.0; 0.0 5.0],\n    nbatch = 2,\n    ninit = 8,\n    nopt = 15,\n    acqmethod = :qEI\n)\n\n# Use with Optim.optimize\nresult = Optim.optimize(objective, method)\n\n# Standard Optim.jl result interface\nprintln(\"Minimizer: \", Optim.minimizer(result))\nprintln(\"Minimum: \", Optim.minimum(result))\nprintln(\"F calls: \", Optim.f_calls(result))\n\n# Additional methods from BoTorch\nif isa(result, BoTorchQBatchState)\n    # Evaluate posterior at any point\n    mean, var = evalposterior(result, [2.0, 3.0])\n    println(\"Posterior at [2,3]: mean=$mean, variance=$var\")\n    \n    # Sample from posterior maximum distribution\n    max_point, std_dev = sampleposteriormin(result, nsamples=1000)\n    println(\"Estimated maximum: $max_point ± $std_dev\")\nend","category":"page"},{"location":"#Advanced-Usage","page":"PyBayesOpt.jl Documentation","title":"Advanced Usage","text":"","category":"section"},{"location":"#Acquisition-Functions","page":"PyBayesOpt.jl Documentation","title":"Acquisition Functions","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"BoTorch supports various acquisition functions:","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"method = BoTorchQBatch(\n    acqmethod = :qLogEI,  # Options: :qEI, :qNEI, :qLogEI, :qLogNEI, :qUCB, :qPI\n    qUCB_beta = 2.5       # Only used for :qUCB\n)","category":"page"},{"location":"#Parallel-Evaluation","page":"PyBayesOpt.jl Documentation","title":"Parallel Evaluation","text":"","category":"section"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"The q-batch approach naturally supports parallel function evaluation:","category":"page"},{"location":"","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.jl Documentation","text":"using Distributed\naddprocs(4)  # Add 4 worker processes\n\n@everywhere using PyBayesOpt\n\nfunction parallel_eval(objective, state)\n    while !finished(state)\n        pts = ask!(state)\n        \n        # Parallel evaluation using pmap\n        values = pmap(i -> objective(pts[:, i]), 1:size(pts, 2))\n        \n        tell!(state, pts, values)\n    end\n    return state\nend\n\nresult = parallel_eval(objective, BoTorchQBatchState(params=method))","category":"page"},{"location":"#API-Reference","page":"PyBayesOpt.jl Documentation","title":"API Reference","text":"","category":"section"},{"location":"#PyBayesOpt","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt","text":"PyBayesOpt\n\n(Image: ci) (Image: ) (Image: )\n\nPyBayesOpt.jl\n\nJulia wrapper to some Bayesian optimization tools available in Python:\n\nBayesianOptimization aka bayes_opt or bayesian-optimization \nA q-batch  Bayesian optimizer implemented using BoTorch\n\nIt uses a very partial implementation of the optimizer interface provided by Optim.jl.\n\nInstallation\n\nPython prerequisites\n\nThe package uses PyCall.jl to access the python code, and therefore requires a python installation which is working well with this package. See requirements.txt for the python packages to be installed.\n\npip install -r requirements.txt\n\nIf you use this under linux, you may have to set the environment variable TORCH_USE_RTLD_GLOBAL=1 in order to avoid some loading problems with the mkl library:\n\nexport TORCH_USE_RTLD_GLOBAL=1\n\nInstallation via PackageNursery registry\n\nThe package can be installed with the Julia package manager in a standard way For the time being, it is registered in the julia package registry https://github.com/j-fu/PackageNursery To add the registry (needed only once), and to install the package,  from the Julia REPL, type ] to enter the Pkg REPL mode and run:\n\npkg> registry add https://github.com/j-fu/PackageNursery\n\nPlease be aware that adding a registry to your Julia installation requires to trust the registry maintainer for handling things in a correct way. In particular, the registry should not register higher versions of packages which are already registered in the Julia General Registry. One can check this by visiting the above mentionend github repository URL and inspecting the contents.\n\nInstallation via  repository URL\n\nusing Pkg\nPkg.add(url=\"https://github.com/j-fu/PyBayesOpt.jl\")\n\nQuick Start\n\nBasic Usage with Optim.jl Interface\n\nusing PyBayesOpt\nusing Optim\n\n# Define a function to minimize\nf(x) = (x[1] - 2.0)^2 + (x[2] - 1.0)^2\n\n# Create optimizer - BoTorch q-batch is recommended\nmethod = BoTorchQBatch(\n    bounds = [0.0 4.0; -1.0 3.0],  # [min max] for each dimension\n    nbatch = 4,      # evaluate 4 points per iteration\n    ninit = 10,      # 10 initialization iterations  \n    nopt = 15        # 15 optimization iterations\n)\n\n# Optimize using standard Optim.jl interface\nresult = optimize(f, method)\n\n# Access results\nprintln(\"Best point: \", result.minimizer)    # [2.0, 1.0]\nprintln(\"Best value: \", result.minimum)      # ≈ 0.0\nprintln(\"Evaluations: \", result.f_calls)     # 100 total evaluations\n\nInteractive Optimization Loop\n\nFor maximum control over the optimization process:\n\nstate = BoTorchQBatchState(params=method)\n\nwhile !finished(state)\n    # Get next batch of points to evaluate\n    candidates = ask!(state)\n    \n    # Evaluate function (can be parallelized)  \n    values = [f(candidates[:, i]) for i in 1:size(candidates, 2)]\n    \n    # Provide results back to optimizer\n    tell!(state, candidates, values)\nend\n\n# Final result\nbest_point, best_value = bestpoint(state)\n\nUsing Benchmark Functions\n\n# Built-in benchmark functions\nbranin = BraninFunction()\nresult = optimize(branin, BoTorchQBatch(bounds=branin.bounds))\n\nackley = AckleyFunction(dim=5)  # 5-dimensional\nresult = optimize(ackley, BoTorchQBatch(bounds=ackley.bounds))\n\nFeatures\n\nAcquisition Functions\n\nBoTorch supports multiple acquisition functions:\n\n:qEI / :qExpectedImprovement - Expected Improvement\n:qLogEI / :qLogExpectedImprovement - Log Expected Improvement (recommended)\n:qNEI / :qNoisyExpectedImprovement - Noisy Expected Improvement\n:qLogNEI / :qLogNoisyExpectedImprovement - Log Noisy Expected Improvement  \n:qUCB / :qUpperConfidenceBound - Upper Confidence Bound\n:qPI / :qProbabilityOfImprovement - Probability of Improvement\n\nParallel Evaluation\n\nThe q-batch approach naturally supports parallel function evaluation:\n\nusing Base.Threads\n\n# In the optimization loop:\nvalues = zeros(nbatch)\n@threads for i in 1:nbatch\n    values[i] = expensive_function(candidates[:, i])\nend\n\nPosterior Analysis\n\nAfter optimization, you can analyze the Gaussian process posterior:\n\n# Evaluate posterior at any point\nmean, variance = evalposterior(result, [1.0, 2.0])\n\n# Sample from posterior maximum distribution\nmax_point, std_dev = samplemaxpost(result, nsamples=1000)\nprintln(\"Estimated global optimum: $max_point ± $std_dev\")\n\nOptimizers\n\nBoTorchQBatch (Recommended)\n\nAdvanced q-batch Bayesian optimization using BoTorch:\n\nBoTorchQBatch(\n    bounds = [-1.0 1.0; -1.0 1.0],    # Optimization bounds\n    nbatch = 4,                        # Batch size\n    ninit = 10,                        # Initialization iterations\n    nopt = 20,                         # Optimization iterations  \n    acqmethod = :qLogEI,               # Acquisition function\n    seed = 1234,                       # Random seed\n    verbose = true,                    # Print progress\n    acq_nrestarts = 20,                # Acquisition optimization restarts\n    acq_nsamples = 512,                # Raw samples for acquisition optimization\n    qUCB_beta = 2.0                    # Beta parameter for qUCB\n)\n\nBayesianOptimization\n\nClassical Bayesian optimization wrapper:\n\nBayesianOptimization(\n    bounds = [-1.0 1.0; -1.0 1.0],    # Optimization bounds\n    ninit = 10,                        # Initial random samples\n    nopt = 50,                         # Optimization iterations\n    verbose = 0,                       # Verbosity level\n    seed = 1                           # Random seed\n)\n\nExamples\n\nSee the examples/ directory for:\n\nquick_start.jl - Basic usage examples  \noptim_interface_example.jl - Comprehensive Optim.jl interface demo\n\nAI usage statement\n\nGithub copilot with Claude Sonnet 4 and GPT-5 was used to design an initial python version of the BoTorch based algorithm, and to brush up documentation and testing infrastructure.\n\n\n\n\n\n","category":"module"},{"location":"#PyBayesOpt.BoTorchQBatch","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.BoTorchQBatch","text":"struct BoTorchQBatch\n\nStruct describing optimization for BoTorch based q-batch Bayesian optimization\n\nFields:\n\nbounds::Matrix{Float64} = [-1 1]' : ndim x 2 matrix of evaluation bounds\nnbatch::Int = 1: batch size for evaluations of black box model\nninit::Int = 10: number of initialization iterations resulting in nbatch*ninit evaluations\nnopt::Int = 10: number of optimization iterations resulting in nbatch*nopt evaluations\nacqmethod::Symbol = :qLogEI: acquisition method.  \nValid metods (see BoTorch docmentation):\n:qEI, :qExpectedImprovement\n:qNEI, :qNoisyExpectedImprovement\n:qLogEI, :qLogExpectedImprovement\n:qLogNEI, :qLogNoisyExpetedImprovement\n:qUCB, :qUpperConfidenceBound\n:qPI, :qProbabilityOfImprovement\nseed::Int = 1234: random seed\nverbose::Bool = true: verbosity\nacq_nrestarts::Int = 20: num_restarts parameter in optimize_acqf\nacq_nsamples::Int = 512: raw_samples parameter in  optimize_acqf\nqUCB_beta::Float64 = 2.0: beta parameter for qUCB_beta acquisition method\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.BoTorchQBatchState","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.BoTorchQBatchState","text":"struct BoTorchQBatchState\n\nState for  BoTorchQBatch, also used as result struct.\n\nAn instance of this state can be used either as the method paramer for the optimize function, or in  user implemented loop as seen below:\n\nstate = BoTorchQBatchState(; params)\nwhile !finished(state)\n    pts = ask!(state)\n    values = zeros(state.params.nbatch)\n    Threads.@threads for i in 1:size(pts, 2)\n          values[i] = func(pts[:, i])\n    end\n    tell!(state, pts, values)\nend\n\nFields\n\nX_ini::Union{Nothing, Matrix{Float64}} = nothing: initialization points\nX_obs::Union{Nothing, PyObject} = nothing: training points\nY_obs::Union{Nothing, PyObject} = nothing: training values\ngpmodel::Union{Nothing, PyObject} = nothing: Gaussian process model\nevaluations_used::Int = 0: number of evaluations done\ninitialization_complete::Bool = false: flag indicating initialization state\noptimization_complete::Bool = false: flag indicating optimization state\ninit_iterations_done::Int = 0: initial iterations performed\noptim_iterations_done::Int = 0: optimization iterations performed\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.BayesianOptimization","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.BayesianOptimization","text":"struct BayesianOptimization\n\nOptimizer wrapping  BayesianOptimization.\n\nFields:\n\n'bounds::Matrix{Float64}'\n'ninit::Int = 10'\n'nopt::Int = 100'\n'verbose::Int = 0'\n'seed::Int = 1'\n\n\n\n\n\n","category":"type"},{"location":"#Optim.optimize","page":"PyBayesOpt.jl Documentation","title":"Optim.optimize","text":"Optim.optimize(func, params::BoTorchQBatch)\n\nMinimize black-box function func using the BoTorchQBatch Bayesian optimization method.\n\nThis provides an Optim.jl-compatible interface so you can call optimize(func, method) or Optim.optimize(func, method) directly. The optimization proceeds in two phases:\n\nInitialization: params.ninit batches of random (Sobol) points are evaluated.\nOptimization: params.nopt iterations of model-based batch acquisition optimization.\n\nMulti-threading: If Threads.nthreads() > 1, function evaluations within a batch are threaded.\n\nArguments\n\nfunc::Function: Objective to minimize (should accept a vector of reals and return a scalar).\nparams::BoTorchQBatch: Configuration object.\n\nReturns\n\nBoTorchQBatchState: Final state which behaves like an Optim.MultivariateOptimizationResults object (supports minimum, minimizer, f_calls, etc.).\n\n\n\n\n\nOptim.optimize(f, params::BayesianOptimization)\n\nMinimize black-box function f using the Python BayesianOptimization library via the BayesianOptimization parameter struct.\n\nWorkflow\n\nConstruct bounds dictionary.\nWrap objective as a maximization target (the Python library maximizes) by negating.\nRun random initialization (ninit) followed by model-guided iterations (nopt).\nConvert best (max) record back to a minimization result.\n\nReturns a BayesianOptimizationResult with minimum, minimizer, and bookkeeping fields.\n\n\n\n\n\n","category":"function"},{"location":"#Base.summary","page":"PyBayesOpt.jl Documentation","title":"Base.summary","text":"Optim.summary(state::BoTorchQBatchState)\n\nPrint a concise human-readable summary of a BoTorchQBatchState including phase, current best point, best value, and (once optimization is complete) posterior statistics at the incumbent.\n\nIntended mainly for verbose / debugging output; called automatically when params.verbose=true.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.initializing","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.initializing","text":"initializing(qbatch_state)\n\nTell if optimization is in initialization state.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.optimizing","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.optimizing","text":"optimizing(qbatch_state)\n\nTell if optimization is in optimization loop.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.finished","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.finished","text":"finished(qbatch_state)\n\nTell if optimization is finished\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.ask!","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.ask!","text":"ask!(qbatch_state)\n\nAsk for a new batch of points to be avaluated. Returns dim x batchsize matrix. At once may generate intial candidates or optimize the acquisition.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.tell!","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.tell!","text":"tell!(qbatch_state, candidates, valus)\n\nProvide newly evaluated candidate points to the optimizatio, update the initialization resp. training set.    \n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.bestpoint","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.bestpoint","text":"bestpoint(qbatch_state)\n\nReturn the best point and function value from the observation set.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.evalposterior","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.evalposterior","text":"evalposterior(qbatch_state, point)\n\nEvaluate posterior at given point. Returns posterior mean and variance.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.sampleposteriormin","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.sampleposteriormin","text":"sampleposteriormin(qbatch_state; nsamples)\n\nSample the posterior minimum using  MaxPosteriorSampling.\n\nReturns the estimated minimum point and the estimated standard deviation of its coordinates. Use evalposterior to obtain the function value in that point.\n\n\n\n\n\n","category":"function"},{"location":"#PyBayesOpt.BayesianOptimizationResult","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.BayesianOptimizationResult","text":"struct BayesianOptimizationResult <: Optim.OptimizationResults\n\nResult struct for BayesianOptimization, compatible with Optim.jl interface.\n\nFields\n\nparams::BayesianOptimization: The optimization parameters used\nf_calls::Int: Number of function evaluations performed\nminimizer::Vector{Float64}: The best point found\nminimum::Float64: The best function value found\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.AbstractBenchmarkFunction","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.AbstractBenchmarkFunction","text":"abstract type AbstractBenchmarkFunction\n\nAbstract base type for benchmark optimization functions.\n\nAll benchmark functions should implement:\n\nFunction call syntax f(x) where x is a vector\nFields bounds, optimal_value, and optionally optimal_point\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.SimpleFunction","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.SimpleFunction","text":"SimpleFunction <: AbstractBenchmarkFunction\n\nA simple quadratic test function for optimization benchmarking.\n\nFunction: f(x) = x[1]² + (x[2] - 1)² - 1\n\nFields\n\nbounds::Matrix{Float64}: Optimization bounds [[-10 10]; [-10 10]]\noptimal_value::Float64: Known optimal value -1.0\noptimal_point::Vector{Float64}: Known optimal point [0.0, 1.0]\n\nExample\n\nsimple_func = SimpleFunction()\nresult = optimize(simple_func, BoTorchQBatch(bounds=simple_func.bounds))\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.BraninFunction","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.BraninFunction","text":"BraninFunction <: AbstractBenchmarkFunction\n\nThe Branin function - a common benchmark for global optimization.\n\nFunction: f(x₁,x₂) = a(x₂ - bx₁² + cx₁ - r)² + s(1-t)cos(x₁) + s where a=1, b=5.1/(4π²), c=5/π, r=6, s=10, t=1/(8π)\n\nFields\n\nbounds::Matrix{Float64}: Optimization bounds [[-5 10]; [0 15]]\noptimal_value::Float64: Global minimum value 0.397887\n\nThe function has 3 global minima at approximately:\n\n(-π, 12.275), (π, 2.275), (9.42478, 2.475)\n\nExample\n\nbranin_func = BraninFunction()\nresult = optimize(branin_func, BoTorchQBatch(bounds=branin_func.bounds))\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.AckleyFunction","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.AckleyFunction","text":"AckleyFunction <: AbstractBenchmarkFunction\n\nThe Ackley function - a widely used benchmark function for global optimization.\n\nFunction: f(x) = -a·exp(-b·√(1/d·∑xᵢ²)) - exp(1/d·∑cos(c·xᵢ)) + a + e where typically a=20, b=0.2, c=2π\n\nFields\n\ndim::Int: Problem dimension (default: 2)  \nbounds::Matrix{Float64}: Optimization bounds (default: [-32.768, 32.768] for each dimension)\noptimal_value::Float64: Global minimum value 0.0\noptimal_point::Vector{Float64}: Global minimum at origin\n\nThe function has a global minimum at the origin with many local minima.\n\nExample\n\nackley_func = AckleyFunction(dim=5)  # 5-dimensional Ackley function\nresult = optimize(ackley_func, BoTorchQBatch(bounds=ackley_func.bounds))\n\n\n\n\n\n","category":"type"},{"location":"#PyBayesOpt.RosenbrockFunction","page":"PyBayesOpt.jl Documentation","title":"PyBayesOpt.RosenbrockFunction","text":"RosenbrockFunction <: AbstractBenchmarkFunction\n\nThe Rosenbrock function (also known as \"Rosenbrock's valley\" or \"banana function\").\n\nFunction: f(x) = ∑[100(xᵢ₊₁ - xᵢ²)² + (1 - xᵢ)²] for i=1 to n-1\n\nFields\n\ndim::Int: Problem dimension (default: 2)\nbounds::Matrix{Float64}: Optimization bounds (default: [-5, 10] for each dimension)  \noptimal_value::Float64: Global minimum value 0.0\noptimal_point::Vector{Float64}: Global minimum at ones(dim)\n\nThe function has a global minimum in a narrow, curved valley making it challenging for optimization algorithms.\n\nExample\n\nrosenbrock_func = RosenbrockFunction(dim=3)  # 3-dimensional Rosenbrock function\nresult = optimize(rosenbrock_func, BoTorchQBatch(bounds=rosenbrock_func.bounds))\n\n\n\n\n\n","category":"type"}]
}
